{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Filters\n",
    "noaa_midwest_codes = [\"11\", \"12\", \"13\", \"14\", \"20\", \"21\", \"23\", \"25\", \"32\", \"33\", \"39\", \"47\"]\n",
    "\n",
    "fips_mapping = {\n",
    "    \"11\": \"17\",  # Illinois\n",
    "    \"12\": \"18\",  # Indiana\n",
    "    \"13\": \"19\",  # Iowa\n",
    "    \"14\": \"20\",  # Kansas\n",
    "    \"20\": \"26\",  # Michigan\n",
    "    \"21\": \"27\",  # Minnesota\n",
    "    \"23\": \"29\",  # Missouri\n",
    "    \"25\": \"31\",  # Nebraska\n",
    "    \"32\": \"38\",  # North Dakota\n",
    "    \"33\": \"39\",  # Ohio\n",
    "    \"39\": \"46\",  # South Dakota\n",
    "    \"47\": \"55\"   # Wisconsin\n",
    "}\n",
    "\n",
    "final_df_cols = ['Year', 'County_Code', 'state_fips']\n",
    "\n",
    "def parse_climdiv_data(file_path, yearly_avg_column_name, midwest_codes=noaa_midwest_codes, final_df_cols=final_df_cols):\n",
    "    # Define the column widths based on the provided positions\n",
    "    column_specs = [\n",
    "        (0, 2),    # STATE-CODE (1-2)\n",
    "        (2, 5),    # DIVISION-NUMBER (3-5)\n",
    "        (5, 7),    # ELEMENT CODE (6-7)\n",
    "        (7, 11),   # YEAR (8-11)\n",
    "        (11, 18),  # JAN-VALUE (12-18)\n",
    "        (18, 25),  # FEB-VALUE (19-25)\n",
    "        (25, 32),  # MAR-VALUE (26-32)\n",
    "        (32, 39),  # APR-VALUE (33-39)\n",
    "        (39, 46),  # MAY-VALUE (40-46)\n",
    "        (46, 53),  # JUNE-VALUE (47-53)\n",
    "        (53, 60),  # JULY-VALUE (54-60)\n",
    "        (60, 67),  # AUG-VALUE (61-67)\n",
    "        (67, 74),  # SEPT-VALUE (68-74)\n",
    "        (74, 81),  # OCT-VALUE (75-81)\n",
    "        (81, 88),  # NOV-VALUE (82-88)\n",
    "        (88, 95),  # DEC-VALUE (89-95)\n",
    "    ]\n",
    "\n",
    "    # Column names\n",
    "    column_names = [\n",
    "        \"State_Code\", \"Division_Number\", \"Element_Code\", \"Year\",\n",
    "        \"Jan_Value\", \"Feb_Value\", \"Mar_Value\", \"Apr_Value\", \"May_Value\", \n",
    "        \"Jun_Value\", \"Jul_Value\", \"Aug_Value\", \"Sep_Value\", \"Oct_Value\", \n",
    "        \"Nov_Value\", \"Dec_Value\"\n",
    "    ]\n",
    "    \n",
    "    # Read the fixed-width file, treating State_Code and Division_Number as strings\n",
    "    df = pd.read_fwf(file_path, colspecs=column_specs, names=column_names, \n",
    "                     dtype={\"State_Code\": str, \"Division_Number\": str})\n",
    "\n",
    "    \n",
    "    # Create a new 'state_fips' column that maps the State_Code using the fips_mapping dictionary\n",
    "    df['state_fips'] = df['State_Code'].map(fips_mapping)  # All values are strings\n",
    "\n",
    "    # Create a new column that combines State_Code and Division_Number\n",
    "    df['County_Code'] = df['state_fips'] + df['Division_Number']\n",
    "\n",
    "    # Convert monthly values to numeric, replacing missing indicators\n",
    "    numeric_columns = column_names[4:]\n",
    "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Handle missing values based on given missing indicators\n",
    "    df.replace({\n",
    "        \"Jan_Value\": {-99.99: None, -9.99: None},\n",
    "        \"Feb_Value\": {-99.99: None, -9.99: None},\n",
    "        \"Mar_Value\": {-99.99: None, -9.99: None},\n",
    "        \"Apr_Value\": {-99.99: None, -9.99: None},\n",
    "        \"May_Value\": {-99.99: None, -9.99: None},\n",
    "        \"Jun_Value\": {-99.99: None, -9.99: None},\n",
    "        \"Jul_Value\": {-99.99: None, -9.99: None},\n",
    "        \"Aug_Value\": {-99.99: None, -9.99: None},\n",
    "        \"Sep_Value\": {-99.99: None, -9.99: None},\n",
    "        \"Oct_Value\": {-99.99: None, -9.99: None},\n",
    "        \"Nov_Value\": {-99.99: None, -9.99: None},\n",
    "        \"Dec_Value\": {-99.99: None, -9.99: None}\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Calculate the yearly average, ignoring missing values\n",
    "    df[yearly_avg_column_name] = df[numeric_columns].mean(axis=1)\n",
    "\n",
    "    midwest_df = df[df['State_Code'].isin(midwest_codes)]\n",
    "\n",
    "    midwest_df_post1960 = midwest_df[midwest_df['Year'] > 1950]\n",
    "\n",
    "    output_columns = final_df_cols + [yearly_avg_column_name]\n",
    "    \n",
    "    return midwest_df_post1960[output_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation_path = 'data/climate_data/climdiv-pcpncy-v1.0.0-20241021.txt'\n",
    "avg_temp_path = 'data/climate_data/climdiv-tmpccy-v1.0.0-20241021.txt'\n",
    "max_temp_path = 'data/climate_data/climdiv-tmaxcy-v1.0.0-20241021.txt'\n",
    "min_temp_path = 'data/climate_data/climdiv-tmincy-v1.0.0-20241021.txt'\n",
    "precip_df = parse_climdiv_data(precipitation_path, \"ann_avg_precip\")\n",
    "avg_temp_df = parse_climdiv_data(avg_temp_path, \"ann_avg_temp\")\n",
    "max_temp_df = parse_climdiv_data(max_temp_path, \"ann_max_temp\")\n",
    "min_temp_df = parse_climdiv_data(min_temp_path, \"ann_min_temp\")\n",
    "# Merge the DataFrames one by one\n",
    "merge_cols = ['Year', 'County_Code', 'state_fips']\n",
    "annual_climate_data_df = precip_df.merge(avg_temp_df, on=merge_cols).merge(max_temp_df, on=merge_cols).merge(min_temp_df, on=merge_cols)\n",
    "\n",
    "annual_climate_data_df = annual_climate_data_df.sort_values(by=['County_Code', 'Year'])\n",
    "\n",
    "# Apply rolling mean within each group\n",
    "rolling_avg_30yr_climate_data_df = (\n",
    "    annual_climate_data_df\n",
    "    .groupby('County_Code')[['Year', 'ann_avg_precip', 'ann_avg_temp', 'ann_max_temp', 'ann_min_temp']]\n",
    "    .apply(lambda x: x.set_index('Year').rolling(window=30).mean())\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annual_climate_data_df = annual_climate_data_df.sort_values(by=['County_Code', 'Year'])\n",
    "\n",
    "# Apply rolling mean within each group\n",
    "rolling_avg_30yr_climate_data_df = (\n",
    "    annual_climate_data_df\n",
    "    .groupby('County_Code')[['Year', 'ann_avg_precip', 'ann_avg_temp', 'ann_max_temp', 'ann_min_temp']]\n",
    "    .apply(lambda x: x.set_index('Year').rolling(window=30).mean())\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "rolling_avg_30yr_climate_data_df[rolling_avg_30yr_climate_data_df['Year'].isin([1980,2023])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point in Time Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vega_datasets import data\n",
    "\n",
    "# Load U.S. states and counties\n",
    "states = data.us_10m.url  # URL for U.S. states\n",
    "counties = data.us_10m.url  # URL for U.S. counties\n",
    "print(states)\n",
    "print(counties)\n",
    "states_gdf = gpd.read_file(states)\n",
    "counties_gdf = gpd.read_file(counties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19550-1980\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vega_datasets import data\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import geopandas as gpd\n",
    "import altair as alt\n",
    "\n",
    "\n",
    "counties = data.us_10m.url  # URL for U.S. counties\n",
    "counties_gdf = gpd.read_file(counties)\n",
    "\n",
    "def load_midwest_counties(db_name, table, counties_gdf):\n",
    "    # SQL query to get distinct state ANSI codes\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        DISTINCT state_ansi\n",
    "    FROM {table} \n",
    "    \"\"\"\n",
    "    \n",
    "    # Connect to the database and execute the query\n",
    "    with sqlite3.connect(db_name) as conn:\n",
    "        check = pd.read_sql(query, conn)\n",
    "\n",
    "    # Extract the list of state ANSI codes\n",
    "    state_ansi_list = check.iloc[:, 0].to_list()\n",
    "    \n",
    "    # Filter the counties GeoDataFrame to include only Midwest counties\n",
    "    midwest_counties_gdf = counties_gdf[\n",
    "        counties_gdf['id'].str[:2].isin(state_ansi_list) & \n",
    "        (counties_gdf['id'].str.len() == 5)\n",
    "    ]\n",
    "\n",
    "    return midwest_counties_gdf\n",
    "\n",
    "# Example usage\n",
    "db_name = 'field_crops.db'\n",
    "table = 'midwest_key_field_crops_cleaned'\n",
    "midwest_counties = load_midwest_counties(db_name, table, counties_gdf)\n",
    "\n",
    "\n",
    "def create_climate_maps(merged_1980, midwest_counties_gdf, climate_metrics):\n",
    "    \n",
    "    # backgrounds\n",
    "    # Define the background chart with a gray fill and black stroke for county borders\n",
    "    county_map_background = alt.Chart(midwest_counties_gdf).mark_geoshape(\n",
    "        fill='lightgray',  # Background color\n",
    "        stroke='black',    # Outline color for counties\n",
    "        strokeWidth=0.5   # Thickness of county borders\n",
    "    ).properties(\n",
    "        width=800,\n",
    "        height=500\n",
    "    ).project('albersUsa')  # Use Albers USA projection\n",
    "\n",
    "    # Load U.S. states data\n",
    "    states = alt.topo_feature(data.us_10m.url, 'states')\n",
    "\n",
    "    # Define state IDs for the Midwestern states\n",
    "    midwestern_state_ids = [17, 18, 19, 20, 26, 27, 29, 31, 38, 39, 46, 55]\n",
    "\n",
    "    # Filter the background chart for the selected states and add black borders\n",
    "    state_map_background = alt.Chart(states).mark_geoshape(\n",
    "        fill=None,\n",
    "        stroke='black',  # Set border color to black\n",
    "        strokeWidth=1.5  # Adjust width as needed\n",
    "    ).transform_filter(\n",
    "        alt.FieldOneOfPredicate(field='id', oneOf=midwestern_state_ids)\n",
    "    ).properties(\n",
    "        width=800,\n",
    "        height=500\n",
    "    ).project('albersUsa')\n",
    "    \n",
    "    \n",
    "    \n",
    "    for metric in climate_metrics:\n",
    "        columns = ['id', 'geometry']\n",
    "        metric_df = merged_1980[columns + [metric]]\n",
    "\n",
    "        # Define the filled map chart\n",
    "        county_map_filled = alt.Chart(metric_df).mark_geoshape(\n",
    "            stroke='black',   # Outline color for counties\n",
    "            strokeWidth=0.5   # Thickness of county borders\n",
    "        ).encode(\n",
    "            color=alt.Color(f'{metric}:Q', scale=alt.Scale(scheme='blues')),  # Sequential color scale for the metric\n",
    "            tooltip=['id:N', f'{metric}:Q']  # Tooltip with county ID and metric value\n",
    "        ).properties(\n",
    "            title=f'Map of Production for {metric}',\n",
    "            width=800,\n",
    "            height=500\n",
    "        ).project('albersUsa')  # Use Albers USA projection\n",
    "\n",
    "        # Layer the filled map on top of the gray background\n",
    "        layered_map = county_map_background + county_map_filled + state_map_background\n",
    "        # Display the chart\n",
    "        layered_map.show()\n",
    "\n",
    "# Example usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vega_datasets import data\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import altair as alt\n",
    "import geopandas as gpd\n",
    "\n",
    "db_name = 'field_crops.db'\n",
    "table = 'midwest_key_field_crops_cleaned'\n",
    "midwest_counties_gdf = load_midwest_counties(db_name, table, counties_gdf)\n",
    "\n",
    "climate_1980 = rolling_avg_30yr_climate_data_df[rolling_avg_30yr_climate_data_df['Year']==1980]\n",
    "climate_1980.rename(columns={'County_Code': 'id'}, inplace=True)\n",
    "\n",
    "# Merge the result DataFrame with the GeoDataFrame\n",
    "merged_1980 = gpd.GeoDataFrame(pd.merge(climate_1980, midwest_counties_gdf, on='id', how='left'))\n",
    "\n",
    "# Set the geometry for the GeoDataFrame\n",
    "merged_1980.set_geometry('geometry', inplace=True)\n",
    "print(len(merged_1980))\n",
    "\n",
    "climate_metrics = ['ann_avg_precip', 'ann_avg_temp', 'ann_max_temp', 'ann_min_temp']\n",
    "create_climate_maps(merged_1980, midwest_counties_gdf, climate_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def calc_differnce(metric):\n",
    "    # Assuming rolling_avg_30yr_climate_data_df is your DataFrame\n",
    "    # Step 1: Filter the DataFrame\n",
    "    filtered_df = rolling_avg_30yr_climate_data_df[rolling_avg_30yr_climate_data_df['Year'].isin([1980, 2023])]\n",
    "\n",
    "    # Step 2: Pivot the DataFrame to make 'Year' columns\n",
    "    pivot_df = filtered_df.pivot(index='County_Code', columns='Year', values=metric)\n",
    "\n",
    "    # Step 3: Calculate the difference between 2023 and 1980\n",
    "    pivot_df[f'{metric}_change'] = pivot_df[2023] - pivot_df[1980]\n",
    "\n",
    "    # Reset index if needed\n",
    "    pivot_df.reset_index(inplace=True)\n",
    "\n",
    "    # Now pivot_df contains the difference for each County_Code\n",
    "    return pivot_df, f'{metric}_change'\n",
    "\n",
    "climate_metrics= ['ann_avg_precip', 'ann_avg_temp', 'ann_max_temp', 'ann_min_temp']\n",
    "def gen_change_df(climate_metrics):\n",
    "    df_list = []\n",
    "    for metric in climate_metrics:\n",
    "        change_df, col_name = calc_differnce(metric)\n",
    "        df_list.append(change_df[['County_Code', col_name]])\n",
    "        #df_list.append(change_df)\n",
    "    return df_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_metrics= ['ann_avg_precip', 'ann_avg_temp', 'ann_max_temp', 'ann_min_temp']\n",
    "df_d  = gen_change_df(climate_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calc_difference(metric, type):\n",
    "    # Assuming rolling_avg_30yr_climate_data_df is your DataFrame\n",
    "    # Step 1: Filter the DataFrame\n",
    "    filtered_df = rolling_avg_30yr_climate_data_df[rolling_avg_30yr_climate_data_df['Year'].isin([1980, 2023])]\n",
    "\n",
    "    # Step 2: Pivot the DataFrame to make 'Year' columns\n",
    "    pivot_df = filtered_df.pivot(index='County_Code', columns='Year', values=metric)\n",
    "\n",
    "    # Step 3: Calculate the difference between 2023 and 1980\n",
    "    if type == \"abs_change\":\n",
    "        pivot_df[f'{metric}_{type}'] = pivot_df[2023] - pivot_df[1980]\n",
    "    if type == \"pct_change\":\n",
    "        pivot_df[f'{metric}_{type}'] = ((pivot_df[2023] - pivot_df[1980]) / pivot_df[1980])*100\n",
    "\n",
    "    # Reset index if needed\n",
    "    pivot_df.reset_index(inplace=True)\n",
    "\n",
    "    # Now pivot_df contains the difference for each County_Code\n",
    "    return pivot_df, f'{metric}_{type}'\n",
    "\n",
    "def gen_change_df(climate_metrics, type):\n",
    "    change_df_list = []\n",
    "    for metric in climate_metrics:\n",
    "        change_df, col_name = calc_difference(metric, type)\n",
    "        # Append the relevant columns to the list\n",
    "        change_df_list.append(change_df[['County_Code', col_name]])\n",
    "    \n",
    "    # Concatenate all DataFrames in the list into a single DataFrame\n",
    "    merge_cols = [ 'County_Code']\n",
    "    change_climate_data_df = change_df_list[0].merge(change_df_list[1], on=merge_cols).merge(change_df_list[2], on=merge_cols).merge(change_df_list[3], on=merge_cols)\n",
    "    change_climate_data_df.reset_index(drop=True, inplace=True)\n",
    "    return change_climate_data_df\n",
    "\n",
    "# Example usage\n",
    "climate_metrics = ['ann_avg_precip', 'ann_avg_temp', 'ann_max_temp', 'ann_min_temp']\n",
    "abs_change_climate_data_df = gen_change_df(climate_metrics,  \"abs_change\")\n",
    "pct_change_climate_data_df = gen_change_df(climate_metrics,  \"pct_change\")\n",
    "abs_change_climate_data_df\n",
    "pct_change_climate_data_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_change_climate_data_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Absolute Change Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'field_crops.db'\n",
    "table = 'midwest_key_field_crops_cleaned'\n",
    "\n",
    "#counties = data.us_10m.url  # URL for U.S. counties\n",
    "counties_gdf = gpd.read_file(counties)\n",
    "midwest_counties_gdf = load_midwest_counties(db_name, table, counties_gdf)\n",
    "\n",
    "\n",
    "abs_change_climate_data_df.rename(columns={'County_Code': 'id'}, inplace=True)\n",
    "\n",
    "# Merge the result DataFrame with the GeoDataFrame\n",
    "change_climate_data_gdf = gpd.GeoDataFrame(pd.merge(abs_change_climate_data_df, midwest_counties_gdf, on='id', how='left'))\n",
    "\n",
    "# Set the geometry for the GeoDataFrame\n",
    "change_climate_data_gdf.set_geometry('geometry', inplace=True)\n",
    "print(len(change_climate_data_gdf))\n",
    "\n",
    "climate_metrics = ['ann_avg_precip_abs_change', 'ann_avg_temp_abs_change', 'ann_max_temp_abs_change', 'ann_min_temp_abs_change']\n",
    "create_climate_maps(change_climate_data_gdf, midwest_counties_gdf, climate_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pct Change Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'field_crops.db'\n",
    "table = 'midwest_key_field_crops_cleaned'\n",
    "version = 'pct'\n",
    "\n",
    "#counties = data.us_10m.url  # URL for U.S. counties\n",
    "\n",
    "midwest_counties_gdf = load_midwest_counties(db_name, table, counties_gdf)\n",
    "\n",
    "\n",
    "pct_change_climate_data_df.rename(columns={'County_Code': 'id'}, inplace=True)\n",
    "\n",
    "# Merge the result DataFrame with the GeoDataFrame\n",
    "change_climate_data_gdf = gpd.GeoDataFrame(pd.merge(pct_change_climate_data_df, midwest_counties_gdf, on='id', how='left'))\n",
    "\n",
    "# Set the geometry for the GeoDataFrame\n",
    "change_climate_data_gdf.set_geometry('geometry', inplace=True)\n",
    "print(len(change_climate_data_gdf))\n",
    "\n",
    "climate_metrics = [f'ann_avg_precip_{version}_change', f'ann_avg_temp_{version}_change', f'ann_max_temp_{version}_change', f'ann_min_temp_{version}_change']\n",
    "create_climate_maps(change_climate_data_gdf, midwest_counties_gdf, climate_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag-data-viz-5d2VArwu-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
